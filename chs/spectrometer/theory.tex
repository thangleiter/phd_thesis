% mainfile: main.tex
\chapter{Theory of spectral noise estimation}\label{ch:speck:theory}
There exists a multitude of methods for estimating noise properties.
\todo{lay out some others}

If the noisy process $x(t)$\sidenote{
    We discuss only classical noise here, meaning $x(t)$ commutes with itself at all times. For descriptions of and spectroscopy protocols for quantum noise refer to \citerr{Clerk2010}{Paz-Silva2017}, for example.
}
has Gaussian statistics, meaning that the value at a given point in time follows a normal distribution with some mean $\mu$ and variance $\sigma^2$ over multiple realizations of the process, it can be fully described by the \gls{psd} $S(\omega)$.\sidenote{
    The term \emph{power spectrum} is often used interchangably. I will do so as well, but emphasize at this point that in digital signal processing in particular, the \emph{spectrum} is a different quantity from the \emph{spectral density}.
}
\todo{maybe a classical signal processing ref?}
For the purpose of noise estimation, the assumption of Gaussianity is a rather weak one as the noise typically arises from a large ensemble of individual fluctuators and is therefore well approximated by a Gaussian distribution by the central limit theorem.\sidenote{
    As an example, consider electronic devices, where voltage noise arises from a large number of defects and other charge traps in oxides being populated and depopulated at certain rates $\gamma$. The ensemble average over these so-called \glspl{tlf} then yields the well-known \oneoverf-like noise spectra.
}
\todo{flesh out this sidenote?}
Even if the process $x(t)$ is not perfectly Gaussian, non-Gaussian contributions can be seen as higher-order contributions if viewed from the perspective of perturbation theory, and therefore the \gls{psd} still captures a significant part of the statistical properties.
For this reason, the \gls{psd} is the central quantity of interest in noise spectroscopy and I will discuss some of its properties in the following.

For real signals $x(t) \in\mathbb{R}$, $S(\omega)$ is an even function and one therefore distinguishes the \emph{two-sided} \gls{psd} $S^{(2)}(\omega)$ defined over $\mathbb{R}$ from the \emph{one-sided} \gls{psd} $S^{(1)}(\omega) = 2 S^{(2)}(\omega)$ defined only over $\mathbb{R}^+$.
Complex signals $x(t)\in\mathbb{C}$ such as those generated by Lock-in amplifiers after demodulation in turn have asymmetric, two-sided \glspl{psd}.
\todo{for,for,for}

\section{Spectrum estimation from time series}\label{sec:speck:theory:time_series_estimation}
To see how the \gls{psd} may be estimated from time-series data, consider a continuous wide-sense stationary\sidenote{
    For a wide-sense stationary (also called weakly stationary) process $x(t)$, the mean is constant and the auto-correlation function $C(t, t') = \ev{x(t)^\ast x(t^\prime)}$ is given by $\ev{x(t)^\ast x(t + \tau)} = \ev{x(0)^\ast x(\tau)}$ with $\tau = t^\prime - t$.
    That is, it is a function of only the time lag $\tau$ and not the absolute point in time.
    For Gaussian processes as discussed here, this also implies stationarity~\cite{Koopmans1995}.
    The property further implies that $C(\tau)$ is an even function.
}
\todo{sketch of auto-correlation function?}
signal in the time domain $x(t)\in\mathbb{C}$ that is observed for some time $T$.
We define the windowed Fourier transform of $x(t)$ and its inverse by
\begin{align}
    \hat{x}_T(\omega) &= \int_{0}^{T}\dd{t} x(t)\e^{\i\omega t} \label{eq:windowed_ft}\\
       \qq*{and} x(t) &= \intinf\ddf{\omega}\hat{x}_T(\omega)\e^{-\i\omega t}, \label{eq:windowed_ft:inverse}
\end{align}
\ie, we assume that outside of the window of observation $x(t)$ is zero.
The auto-correlation function of $x(t)$ is given by
\begin{align}
    C(\tau) &= \expval{x(t)^\ast x(t + \tau)} \label{eq:autocorrelation}\\
            &= \lim_{T\to\infty} \frac{1}{T}\int_0^T\dd{t} x(t)^{\ast} x(t + \tau),
\end{align}
where $\expval{\placeholder}$ is the ensemble average over multiple realizations of the process and the last equality holds true for ergodic processes.
Expressing $x(t)$ in terms of its Fourier representation (\cref{eq:windowed_ft}) and reordering the integrals, we get\sidenote{
    Mathematicians might at this point argue the integrability of $x(t)$, but as we deal with physical processes with finite bandwidth (and have no shame), we do not.
}
\begin{align}
    C(\tau) &= \lim_{T\to\infty}\frac{1}{T}\int_0^T\dd{t}
                \intinf\ddf{\omega}\hat{x}_T(\omega)^{\ast}\e^{\i\omega t}
                \intinf\ddf{\omega^\prime}\hat{x}_T(\omega^\prime)\e^{-\i\omega^\prime (t + \tau)}  \\
            &= \lim_{T\to\infty}\frac{1}{T}\intinf\ddf{\omega}\intinf\ddf{\omega^\prime}
                \hat{x}_T(\omega)^{\ast}\hat{x}_T(\omega^\prime)\e^{-\i\omega^\prime\tau}
                \int_0^T\dd{t}\e^{\i t (\omega - \omega^\prime)} \label{eq:autocorrelation:fourier}
\end{align}
The innermost integral approaches a $\delta$-function for large $T$,\sidenote{
    Note that, because $x(t)$ is wide-sense stationary, we may shift the limits of integration $\int_{0}^{T}\to\int_{-\flatfrac{T}{2}}^{+\flatfrac{T}{2}}$.
}
allowing us to further simplify this under the limit as
\begin{align}
    C(\tau) &= \lim_{T\to\infty} \frac{1}{T}\intinf\ddf{\omega}\intinf\ddf{\omega^\prime}
                \hat{x}_T(\omega)^{\ast}\hat{x}_T(\omega^\prime)
                \e^{-\i\omega^\prime\tau}\delta(\omega - \omega^\prime)\\
            &= \lim_{T\to\infty}\frac{1}{T}
                \intinf\ddf{\omega}\abs{\hat{x}_T(\omega)}^2 \e^{-\i\omega\tau} \\
            &= \intinf\ddf{\omega} S(\omega) \e^{-\i\omega\tau} \label{eq:wiener_khinchin}
\end{align}
with the \gls{psd}
\begin{align}
    S(\omega) &= \lim_{T\to\infty}\frac{1}{T}\abs{\hat{x}_T(\omega)}^2 \label{eq:psd:definition}\\
              &= \intinf\dd{\tau} C(\tau)\e^{\i\omega\tau}
\end{align}
\Cref{eq:wiener_khinchin} is the Wiener-Khinchin theorem that states that the auto-correlation function $C(\tau)$ and the \gls{psd} $S(\omega)$ are Fourier-transform pairs~\cite{Koopmans1995}.
Furthermore, defining the latter through \cref{eq:psd:definition} gives us an intuitive picture of the \gls{psd} if we recall Parseval's theorem,
\begin{align}\label{eq:parseval}
    \intinf\ddf{\omega}\frac{1}{T}\abs{\hat{x}_T(\omega)}^2 = \frac{1}{T}\intinf\dd{t}\abs{x(t)}^2.
\end{align}
That is, the total power $P$ contained in the signal $x(t)$ is given by integrating over the \gls{psd}.
Similarly, the power contained in a band of frequencies $[\omega_1, \omega_2]$ is given by
\begin{align}
    P(\omega_1, \omega_2) &= \text{RMS}(\omega_1, \omega_2)^2 \\
                          &= \int_{\omega_1}^{\omega_2}\ddf{\omega} S(\omega) \label{eq:psd:bandpower}
\end{align}
where $\text{RMS}(\omega_1, \omega_2)$ is the root-mean-square within this frequency band.
These relations are helpful when analyzing noise \glspl{psd} to gauge the relative weight of contributions from different frequency bands to the total noise power.

\Cref{eq:psd:definition} represents the starting point for the experimental spectrum estimation procedure.
Instead of a continuous signal $x(t), t\in [0, T]$, consider its discretized version
\begin{align}\label{eq:signal:discrete}
    x_n \qc n\in\lbrace 0, 1, \dotsc, N - 1\rbrace
\end{align}
defined at times $t_n = n\Delta t$ with $T = N\Delta t$ and where $\Delta t = \fs\inverse$ is the sampling interval (the inverse of the sampling frequency \fs).
Invoking the ergodic theorem, we can replace the long-term average in \cref{eq:psd:definition} by the ensemble average over $M$ realizations of the noisy signal $x_n$ and write
\begin{align}\label{eq:psd:definition:discrete}
    S_n = \lim_{M\to\infty}\frac{1}{M} \sum_{i=0}^M \abs{\hat{x}_n}^2,
\end{align}
where $\hat{x}_n$ is the discrete Fourier transform of $x_n$ and $S_n$ the \gls{psd} sampled at the discrete frequencies $\omega_n\in \flatfrac{2\pi}{T}\times\lbrace\flatfrac{-N}{2}, \flatfrac{-N}{2} + 1, \dotsc, \flatfrac{N}{2}\rbrace = 2\pi\times\lbrace\flatfrac{-\fs}{2}, \dotsc, \flatfrac{\fs}{2}\rbrace$.\sidenote{
    We blithely disregard integer algebra issues occuring here for conciseness and leave it as an exercise for the reader to figure out what the exact bounds of the set of $\omega_n$ are.
}

